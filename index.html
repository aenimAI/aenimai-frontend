<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>aenimAI Realtime Voice Chat</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #121212;
      color: #f5f5f5;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      height: 100vh;
      margin: 0;
      text-align: center;
    }
    h1 { color: #1db954; margin-bottom: 8px; }
    #status { color: #ccc; font-size: 16px; margin-bottom: 8px; }

    #inputRow{
      display:flex;
      gap:8px;
      align-items:center;
      justify-content:center;
      margin-top: 12px;
      width: min(560px, 92vw);
    }
    #textInput{
      flex:1;
      min-width: 180px;
      padding: 10px 12px;
      border-radius: 10px;
      border: 1px solid #333;
      background: rgba(0,0,0,0.35);
      color: #f5f5f5;
      outline: none;
    }
    #sendText{
      padding: 10px 14px;
      border-radius: 10px;
      border: 1px solid #2b2b2b;
      background: #1db954;
      color: #0b0b0b;
      cursor: pointer;
      font-weight: 600;
    }
    #sendText:disabled{
      opacity: 0.5;
      cursor: not-allowed;
    }
    #log {
      position: fixed;
      bottom: 10px; left: 10px; right: 10px;
      max-height: 35vh; overflow-y: auto;
      font-size: 11px;
      background: rgba(0,0,0,0.75);
      padding: 8px; border-radius: 8px;
      border: 1px solid #333;
      text-align: left; white-space: pre-wrap;
    }
  </style>
</head>
<body>
  <h1>üéôÔ∏è aenimAI Realtime Voice Chat</h1>
  <p id="status">Initializing...</p>

  <div id="inputRow">
    <input id="textInput" type="text" placeholder="Type a message‚Ä¶" autocomplete="off" />
    <button id="sendText" disabled>Send</button>
  </div>

  <div id="log"></div>

  <script>
    const N8N_SESSION_WEBHOOK = "https://aenimai.app.n8n.cloud/webhook/realtime-ai";
    const N8N_LOG_WEBHOOK     = "https://aenimai.app.n8n.cloud/webhook/get-logs";

    const SESSION_ID = "sess_" + Math.random().toString(36).slice(2) + "_" + Date.now();

    const statusEl = document.getElementById("status");
    const logEl    = document.getElementById("log");

    const textInputEl = document.getElementById("textInput");
    const sendBtnEl   = document.getElementById("sendText");

    function setStatus(t){ statusEl.innerText = t; }
    function appendLog(t){
      const ts = new Date().toISOString().slice(11,19);
      logEl.textContent += `[${ts}] ${t}\n`;
      logEl.scrollTop = logEl.scrollHeight;
    }

    async function sendToN8N(role, text){
      if(!text) return;
      const body = {
        source: "aenimAI-frontend",
        session_id: SESSION_ID,
        timestamp: new Date().toISOString(),
        role, text
      };
      try{
        await fetch(N8N_LOG_WEBHOOK,{
          method:"POST",
          headers:{ "Content-Type":"application/json" },
          body: JSON.stringify(body)
        });
        appendLog(`üì§ Sent to n8n [${role}]: ${text}`);
      }catch(err){
        appendLog("‚ö†Ô∏è Failed to send: "+err.message);
      }
    }

    async function getSession(){
      appendLog("üîÑ Requesting session from n8n...");
      const r = await fetch(N8N_SESSION_WEBHOOK);
      const txt = await r.text();
      appendLog("üì• Session: "+txt.slice(0,200)+"...");
      return JSON.parse(txt);
    }

    async function initRealtime(){
      try{
        const session = await getSession();
        const token = session.client_secret.value;
        const model = session.model;

        const pc = new RTCPeerConnection();

        const remoteAudio = document.createElement("audio");
        remoteAudio.autoplay = true;
        pc.ontrack = e => { remoteAudio.srcObject = e.streams[0]; };

        const mic = await navigator.mediaDevices.getUserMedia({audio:true});
        mic.getTracks().forEach(t=>pc.addTrack(t,mic));

        const dc = pc.createDataChannel("oai-events");

        // ‚úÖ Request assistant response explicitly (text OR audio)
        function requestAssistantResponse(modalities){
          if(!dc || dc.readyState !== "open"){
            appendLog("‚ö†Ô∏è Data channel not open yet ‚Äî can't request response.");
            return;
          }
          dc.send(JSON.stringify({
            type: "response.create",
            response: { modalities } // ["text"] or ["audio"]
          }));
        }

        dc.onopen = ()=>{
          appendLog("üì° Data channel open");
          sendBtnEl.disabled = false;
        };
        dc.onerror=(e)=>appendLog("‚ö†Ô∏è Data channel error "+(e?.message || ""));

        // ‚úÖ Typed input => create user msg + request TEXT response
        function sendTypedText(){
          const text = (textInputEl.value || "").trim();
          if(!text) return;

          if(!dc || dc.readyState !== "open"){
            appendLog("‚ö†Ô∏è Not connected yet ‚Äî can't send typed text.");
            return;
          }

          // Create the user message
          dc.send(JSON.stringify({
            type: "conversation.item.create",
            item: {
              type: "message",
              role: "user",
              content: [{ type: "input_text", text }]
            }
          }));

          appendLog("üßë User (typed): " + text);
          sendToN8N("user", text);

          // Now force a TEXT-only response
          requestAssistantResponse(["text"]);

          textInputEl.value = "";
          textInputEl.focus();
        }

        sendBtnEl.addEventListener("click", sendTypedText);
        textInputEl.addEventListener("keydown", (e)=>{
          if(e.key === "Enter") sendTypedText();
        });

        // Optional: buffer to make text deltas look nicer (instead of one log line per delta)
        let textBuffer = "";
        let textStreaming = false;

        dc.onmessage = async (event)=>{
          console.log("FULL RAW:", event.data);
          let msg;
          try { msg = JSON.parse(event.data); }
          catch { return; }

          const type = msg.type || "";

          // ===============================
          // 1) USER SPEECH TRANSCRIPTION
          // ===============================
          if(type === "conversation.item.input_audio_transcription.completed"){
            const text = (msg.transcript || "").trim();
            if(text){
              appendLog("üßë User: " + text);
              await sendToN8N("user", text);

              // ‚úÖ Spoken input => request AUDIO response
              requestAssistantResponse(["audio"]);
            }
          }

          // ===============================
          // 2) ASSISTANT TEXT EVENTS (typed mode)
          // ===============================
          if(type === "response.text.delta"){
            const delta = msg.delta || "";
            if(delta){
              textBuffer += delta;
              if(!textStreaming){
                textStreaming = true;
                appendLog("ü§ñ Assistant (typing): " + textBuffer);
              } else {
                // replace last line behavior is not trivial in plain text log;
                // so we just append periodically or on done. We'll keep it simple:
              }
            }
          }

          if(type === "response.text.done"){
            const text = (msg.text || textBuffer || "").trim();
            if(text){
              appendLog("ü§ñ Assistant: " + text);
              await sendToN8N("assistant", text);
            }
            textBuffer = "";
            textStreaming = false;
          }

          // ===============================
          // 3) ASSISTANT AUDIO TRANSCRIPT EVENTS (voice mode)
          // ===============================
          if(type === "response.output_item.done"){
            if(!msg.item || !msg.item.content) return;
            for(const c of msg.item.content){
              if(c.type === "audio" && c.transcript){
                const text = c.transcript.trim();
                appendLog("ü§ñ Assistant (voice): " + text);
                await sendToN8N("assistant", text);
              }
            }
          }
        };

        // WebRTC offer ‚Üí OpenAI
        const offer = await pc.createOffer();
        await pc.setLocalDescription(offer);

        const res = await fetch(
          `https://api.openai.com/v1/realtime?model=${model}`,
          {
            method:"POST",
            headers:{
              "Content-Type":"application/sdp",
              Authorization:`Bearer ${token}`
            },
            body: offer.sdp
          }
        );

        const ans = await res.text();
        await pc.setRemoteDescription({type:"answer", sdp:ans});

        setStatus("üü¢ Connected ‚Äî speak now (or type below)!");
        appendLog("‚úÖ Connected to OpenAI realtime");

      }catch(err){
        setStatus("‚ùå "+err.message);
        appendLog("‚ùå Init error: "+err.message);
      }
    }

    initRealtime();
  </script>
</body>
</html>
