<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>aenimAI Voice Chat</title>
  <style>
    /* Reset some basic styles for consistency */
    html, body {
      margin: 0; padding: 0;
      background: #000;  /* base black background */
      color: #fff;       /* default text color in white */
      font-family: sans-serif;
      height: 100%; width: 100%;
      overflow: hidden;  /* hide scrollbars (single-page app) */
    }
    /* Full-page container */
    .container {
      position: relative;
      width: 100%;
      height: 100%;
      /* Use flex to center content */
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      /* Dark glassy background effect: a subtle radial glow */
      background: radial-gradient(circle at 50% 50%, rgba(255,255,255,0.05), rgba(0,0,0,0) 70%);
      /* The above gradient adds a faint white glow at center */
      /* (We could also add backdrop-filter blur on an overlay for more glass effect) */
    }
    /* Glowing Eye Logo */
    .logo {
      width: clamp(150px, 25vw, 300px);  /* responsive size: min 150px, max 300px, or 25% viewport width */
      /* Centered within its container (container uses flex, so it's already centered horizontally) */
      /* Pulsing glow effect around the logo */
      filter: drop-shadow(0 0 8px rgba(255,255,255,0.8));
      /* drop-shadow creates a glow based on image shape */
      animation: logoPulse 2.5s ease-in-out infinite;
    }
    @keyframes logoPulse {
      0%, 100% { filter: drop-shadow(0 0 8px rgba(255,255,255,0.5)); }
      50% { filter: drop-shadow(0 0 20px rgba(255,255,255,1)); }
    }
    /* Waveform bars container (positioned behind the logo) */
    .waveforms {
      position: absolute;
      top: 50%; left: 50%;
      transform: translate(-50%, -50%);  /* center the waveform group on screen */
      display: flex;
      gap: 4px;
      /* We will make the waveform container a thin horizontal strip centered at the logo */
      height: 4px;  /* base height of bars (they will scale around this baseline) */
      width: 60vw;  /* span a good width of viewport for a cinematic look */
      max-width: 800px;
      /* overflow visible allows bars to extend beyond this baseline strip when scaled */
      overflow: visible;
      pointer-events: none;  /* click-through, so it doesn't block clicking the logo or controls */
    }
    .waveforms div {
      background: #fff;
      width: 4px;
      border-radius: 2px;
      /* Each bar initially is as tall as the .waveforms container (baseline height) */
      height: 100%;
      transform-origin: center center;  /* scale from center (so bars expand up & down) */
      /* We will apply animations via state classes on .container */
    }
    /* Idle state – subtle small oscillation */
    .container.idle .waveforms div {
      animation: waveIdle 3s ease-in-out infinite;
    }
    @keyframes waveIdle {
      0%, 100% { transform: scaleY(1); }
      50% { transform: scaleY(1.5); }  /* gentle 50% growth */
    }
    /* Listening state – a bit faster and taller pulsation */
    .container.listening .waveforms div {
      animation: waveListen 1.8s ease-in-out infinite;
    }
    @keyframes waveListen {
      0%, 100% { transform: scaleY(1); }
      50% { transform: scaleY(3); }  /* more noticeable pulse */
    }
    /* Speaking state – lively, irregular waveform animation */
    .container.speaking .waveforms div {
      animation: waveSpeak 1s ease-in-out infinite;
    }
    @keyframes waveSpeak {
      0%   { transform: scaleY(1); }
      25%  { transform: scaleY(4); }
      50%  { transform: scaleY(0.5); }
      75%  { transform: scaleY(3); }
      100% { transform: scaleY(1); }
    }
    /* Staggered animation delays for waveform bars to create a traveling wave pattern */
    .container.speaking .waveforms div:nth-child(5n+1) { animation-delay: 0s; }
    .container.speaking .waveforms div:nth-child(5n+2) { animation-delay: 0.1s; }
    .container.speaking .waveforms div:nth-child(5n+3) { animation-delay: 0.2s; }
    .container.speaking .waveforms div:nth-child(5n+4) { animation-delay: 0.3s; }
    .container.speaking .waveforms div:nth-child(5n+5) { animation-delay: 0.4s; }
    /* (For idle/listening, no stagger – all bars pulse in sync for a calmer look) */
    /* Control buttons styling */
    .controls {
      position: absolute;
      bottom: 5%; 
      display: flex;
      gap: 20px;
      /* center horizontally */
      left: 50%; transform: translateX(-50%);
    }
    .controls button {
      background: rgba(255,255,255,0.1);
      backdrop-filter: blur(5px);
      border: 1px solid rgba(255,255,255,0.3);
      color: #fff;
      font-size: 1rem;
      padding: 0.5em 1em;
      border-radius: 5px;
      cursor: pointer;
    }
    .controls button:hover {
      background: rgba(255,255,255,0.2);
    }
    .controls button:disabled {
      opacity: 0.5; cursor: not-allowed;
    }
    /* Transcript text area (for debugging or optional display) */
    .transcript {
      position: absolute;
      top: 10%;
      width: 80%;
      max-width: 600px;
      left: 50%; transform: translateX(-50%);
      text-align: center;
      font-size: 0.9rem;
      color: rgba(255,255,255,0.8);
    }
  </style>
</head>
<body>
  <div class="container idle" id="app">  <!-- container with state class -->
    <!-- Glowing eye logo -->
    <img src="logoAE.png" alt="aenimAI" class="logo" id="logo" />
    <!-- Waveform bars (20 bars for example) -->
    <div class="waveforms" id="waveform">
      <!-- We generate multiple span/div bars for the waveform -->
      ${Array(20).fill('<div></div>').join('')}
    </div>
    <!-- Controls: Start/Stop/Reset -->
    <div class="controls">
      <button id="startBtn">Start</button>
      <button id="stopBtn" disabled>Stop</button>
      <button id="resetBtn">Reset</button>
    </div>
    <!-- Transcript display (optional, can be hidden if not needed) -->
    <div class="transcript" id="transcript"></div>
  </div>

  <script>
    /* JavaScript: WebRTC/OpenAI voice chat functionality */
    const startBtn = document.getElementById('startBtn');
    const stopBtn  = document.getElementById('stopBtn');
    const resetBtn = document.getElementById('resetBtn');
    const transcriptEl = document.getElementById('transcript');
    const appContainer = document.getElementById('app');

    // State variables
    let socket = null;
    let mediaStream = null;
    let mediaProcessor = null;
    let audioContext = null;
    let sourceNode = null;
    let outputBufferQueue = [];  // queue for incoming audio data
    let outputSourcePlaying = false;
    let nextAudioTime = 0;

    // Helper: switch UI state
    function setState(state) {
      appContainer.classList.remove('idle','listening','speaking');
      appContainer.classList.add(state);
    }

    // Play an incoming audio chunk (Float32Array PCM data)
    function playAudioChunk(chunk) {
      if (!audioContext) return;
      // Create audio buffer from PCM float data (assuming 16kHz sample rate from server for this example)
      const sampleRate = 16000;
      const audioBuffer = audioContext.createBuffer(1, chunk.length, sampleRate);
      audioBuffer.getChannelData(0).set(chunk);
      const bufferSource = audioContext.createBufferSource();
      bufferSource.buffer = audioBuffer;
      bufferSource.connect(audioContext.destination);
      // Schedule playback seamlessly
      if (nextAudioTime < audioContext.currentTime) {
        nextAudioTime = audioContext.currentTime;
      }
      bufferSource.start(nextAudioTime);
      nextAudioTime += audioBuffer.duration;
      bufferSource.onended = () => {
        // When finished playing, if no more audio scheduled, revert to listening state
        if (Math.abs(nextAudioTime - audioContext.currentTime) < 0.1) {  // no pending audio
          setState('listening');
        }
      };
    }

    // Initialize microphone capture and websocket connection
    async function startVoiceChat() {
      if (socket && socket.readyState === WebSocket.OPEN) return;
      // Request mic access
      mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      // Create a ScriptProcessor node to handle audio chunks (for simplicity)
      sourceNode = audioContext.createMediaStreamSource(mediaStream);
      mediaProcessor = audioContext.createScriptProcessor(4096, 1, 1);
      sourceNode.connect(mediaProcessor);
      // On audio process, send audio data via WebSocket
      mediaProcessor.onaudioprocess = (e) => {
        if (!socket || socket.readyState !== WebSocket.OPEN) return;
        const audioData = e.inputBuffer.getChannelData(0);
        // Copy the float data and send as ArrayBuffer
        try {
          socket.send(audioData.buffer.slice(0));  // send raw PCM float data
        } catch (err) {
          console.error("WebSocket send error:", err);
        }
      };

      // Connect to server via WebSocket (assume same host, e.g. ws://localhost:8000/ws)
      const wsUrl = (location.protocol === 'https:' ? 'wss://' : 'ws://') + location.host + '/ws';
      socket = new WebSocket(wsUrl);
      socket.binaryType = 'arraybuffer';
      socket.onopen = () => {
        console.log("WebSocket connected");
        // Start processing audio once websocket is open
        mediaProcessor.connect(audioContext.destination);  // we don't actually output mic to speakers, but connecting processor starts it
        setState('listening');
        startBtn.disabled = true;
        stopBtn.disabled = false;
      };
      socket.onmessage = (event) => {
        if (typeof event.data === 'string') {
          // Handle text message (transcript or system message)
          let msg;
          try { msg = JSON.parse(event.data); } catch { msg = { text: event.data }; }
          if (msg.text) {
            // Update transcript display
            transcriptEl.textContent = msg.text;
          }
          if (msg.event === 'speech_start') {
            // AI is starting to speak
            setState('speaking');
          }
          if (msg.event === 'speech_end') {
            // AI finished speaking (if server indicates); we'll switch back to listening at audio end
            // (The onended of bufferSource will handle state reset to listening)
          }
        } else {
          // Binary data: treat as audio chunk from AI
          const audioData = new Float32Array(event.data);
          setState('speaking');
          playAudioChunk(audioData);
        }
      };
      socket.onclose = () => {
        console.log("WebSocket closed");
        stopVoiceChat();
      };
      socket.onerror = (err) => {
        console.error("WebSocket error:", err);
      };
    }

    function stopVoiceChat() {
      // Stop capturing audio and close connection
      if (mediaProcessor) {
        mediaProcessor.disconnect();
        mediaProcessor = null;
      }
      if (sourceNode) {
        sourceNode.disconnect();
        sourceNode = null;
      }
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }
      if (socket) {
        socket.close();
        socket = null;
      }
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
      setState('idle');
      startBtn.disabled = false;
      stopBtn.disabled = true;
    }

    function resetConversation() {
      // Optionally, send a reset command to server or just clear UI
      transcriptEl.textContent = "";
      // If socket is open, maybe notify server to reset state (depends on server implementation)
      if (socket && socket.readyState === WebSocket.OPEN) {
        socket.send(JSON.stringify({ command: "reset" }));
      }
    }

    // Hook up button events
    startBtn.addEventListener('click', () => {
      startVoiceChat().catch(err => {
        console.error("Failed to start voice chat:", err);
        alert("Microphone access is required to start voice chat.");
      });
    });
    stopBtn.addEventListener('click', () => {
      stopVoiceChat();
    });
    resetBtn.addEventListener('click', () => {
      resetConversation();
    });
  </script>
</body>
</html>
