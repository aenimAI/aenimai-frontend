<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>aenimAI Realtime Voice Chat</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #121212;
      color: #f5f5f5;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      height: 100vh;
      margin: 0;
      text-align: center;
    }
    h1 { color: #1db954; margin-bottom: 8px; }
    #status { color: #ccc; font-size: 16px; margin-bottom: 8px; }

    #controlsRow{
      display:flex;
      gap:10px;
      align-items:center;
      justify-content:center;
      margin-top:12px;
      width:min(720px, 92vw);
    }
    #textInput{
      flex:1;
      min-width:180px;
      padding:10px 12px;
      border-radius:10px;
      border:1px solid #333;
      background: rgba(0,0,0,0.35);
      color:#f5f5f5;
      outline:none;
    }
    button{
      padding:10px 14px;
      border-radius:10px;
      border:1px solid #2b2b2b;
      cursor:pointer;
      font-weight:600;
      white-space:nowrap;
    }
    #sendText{
      background:#1db954;
      color:#0b0b0b;
    }
    #sendText:disabled,
    #micToggle:disabled{
      opacity:0.5;
      cursor:not-allowed;
    }
    #micToggle{
      background:#2a2a2a;
      color:#f5f5f5;
    }
    #micToggle.on{
      background:#1db954;
      color:#0b0b0b;
    }

    #log {
      position: fixed;
      bottom: 10px; left: 10px; right: 10px;
      max-height: 35vh; overflow-y: auto;
      font-size: 11px;
      background: rgba(0,0,0,0.75);
      padding: 8px; border-radius: 8px;
      border: 1px solid #333;
      text-align: left; white-space: pre-wrap;
    }
  </style>
</head>

<body>
  <h1>üéôÔ∏è aenimAI Realtime Voice Chat</h1>
  <p id="status">Initializing...</p>

  <div id="controlsRow">
    <input id="textInput" type="text" placeholder="Type a message‚Ä¶" autocomplete="off" />
    <button id="sendText" disabled>Send</button>
    <button id="micToggle" disabled>üéôÔ∏è Talk</button>
  </div>

  <div id="log"></div>

  <script>
    const N8N_SESSION_WEBHOOK = "https://aenimai.app.n8n.cloud/webhook/realtime-ai";
    const N8N_LOG_WEBHOOK     = "https://aenimai.app.n8n.cloud/webhook/get-logs";

    const SESSION_ID = "sess_" + Math.random().toString(36).slice(2) + "_" + Date.now();

    const statusEl = document.getElementById("status");
    const logEl    = document.getElementById("log");

    const textInputEl = document.getElementById("textInput");
    const sendBtnEl   = document.getElementById("sendText");
    const micBtnEl    = document.getElementById("micToggle");

    function setStatus(t){ statusEl.innerText = t; }
    function appendLog(t){
      const ts = new Date().toISOString().slice(11,19);
      logEl.textContent += `[${ts}] ${t}\n`;
      logEl.scrollTop = logEl.scrollHeight;
    }

    async function sendToN8N(role, text){
      if(!text) return;
      const body = {
        source: "aenimAI-frontend",
        session_id: SESSION_ID,
        timestamp: new Date().toISOString(),
        role, text
      };
      try{
        await fetch(N8N_LOG_WEBHOOK,{
          method:"POST",
          headers:{ "Content-Type":"application/json" },
          body: JSON.stringify(body)
        });
        appendLog(`üì§ Sent to n8n [${role}]: ${text}`);
      }catch(err){
        appendLog("‚ö†Ô∏è Failed to send: "+err.message);
      }
    }

    async function getSession(){
      appendLog("üîÑ Requesting session from n8n...");
      const r = await fetch(N8N_SESSION_WEBHOOK);
      const txt = await r.text();
      appendLog("üì• Session: " + txt.slice(0,200) + "...");
      return JSON.parse(txt);
    }

    async function initRealtime(){
      try{
        const session = await getSession();
        const token = session.client_secret.value;
        const model = session.model;

        const pc = new RTCPeerConnection();

        // ‚úÖ Remote audio element (assistant voice). Append + force play to avoid autoplay issues.
        const remoteAudio = document.createElement("audio");
        remoteAudio.autoplay = true;
        remoteAudio.muted = false;
        remoteAudio.playsInline = true;
        remoteAudio.style.display = "none";
        document.body.appendChild(remoteAudio);

        pc.ontrack = (e) => {
          remoteAudio.srcObject = e.streams[0];
          remoteAudio.play().catch(err => appendLog("üîá Audio play blocked: " + err.message));
        };

        // ‚úÖ Reserve audio m-line in SDP (no mic permission prompt)
        const audioTransceiver = pc.addTransceiver("audio", { direction: "sendrecv" });
        const audioSender = audioTransceiver.sender;

        let micStream = null;
        let micEnabled = false;

        async function enableMic(){
          if(micEnabled) return;
          try{
            micStream = await navigator.mediaDevices.getUserMedia({ audio:true });
            const track = micStream.getAudioTracks()[0];
            await audioSender.replaceTrack(track);

            micEnabled = true;
            micBtnEl.classList.add("on");
            micBtnEl.textContent = "üî¥ Talking";
            appendLog("üéôÔ∏è Mic enabled");
          }catch(err){
            appendLog("‚ùå Mic error: " + err.message);
          }
        }

        async function disableMic(){
          if(!micEnabled) return;

          try{ await audioSender.replaceTrack(null); } catch(e){}

          if(micStream){
            micStream.getTracks().forEach(t => t.stop());
          }
          micStream = null;

          micEnabled = false;
          micBtnEl.classList.remove("on");
          micBtnEl.textContent = "üéôÔ∏è Talk";
          appendLog("üéôÔ∏è Mic disabled");
        }

        const dc = pc.createDataChannel("oai-events");

        function requestAssistantResponse(modalities){
          if(!dc || dc.readyState !== "open"){
            appendLog("‚ö†Ô∏è Data channel not open yet ‚Äî can't request response.");
            return;
          }
          dc.send(JSON.stringify({
            type: "response.create",
            response: { modalities } // ["text"] or ["audio"]
          }));
        }

        // ‚úÖ Mic toggle: also "unlock" audio output on user gesture
        micBtnEl.addEventListener("click", ()=>{
          remoteAudio.play().catch(()=>{});
          if(micEnabled) disableMic();
          else enableMic();
        });

        dc.onopen = ()=>{
          appendLog("üì° Data channel open");
          sendBtnEl.disabled = false;
          micBtnEl.disabled  = false;
        };
        dc.onerror = (e)=>appendLog("‚ö†Ô∏è Data channel error "+(e?.message || ""));

        function sendTypedText(){
          const text = (textInputEl.value || "").trim();
          if(!text) return;

          if(!dc || dc.readyState !== "open"){
            appendLog("‚ö†Ô∏è Not connected yet ‚Äî can't send typed text.");
            return;
          }

          dc.send(JSON.stringify({
            type: "conversation.item.create",
            item: {
              type: "message",
              role: "user",
              content: [{ type: "input_text", text }]
            }
          }));

          appendLog("üßë User (typed): " + text);
          sendToN8N("user", text);

          requestAssistantResponse(["text"]);

          textInputEl.value = "";
          textInputEl.focus();
        }

        sendBtnEl.addEventListener("click", sendTypedText);
        textInputEl.addEventListener("keydown", (e)=>{
          if(e.key === "Enter") sendTypedText();
        });

        let textBuffer = "";

        dc.onmessage = async (event)=>{
          console.log("FULL RAW:", event.data);
          let msg;
          try { msg = JSON.parse(event.data); }
          catch { return; }

          const type = msg.type || "";

          // 1) User speech -> transcription
          if(type === "conversation.item.input_audio_transcription.completed"){
            const text = (msg.transcript || "").trim();
            if(text){
              appendLog("üßë User: " + text);
              await sendToN8N("user", text);

              // Only speak back if mic mode is enabled
              if(micEnabled){
                requestAssistantResponse(["audio"]);
              }
            }
          }

          // 2) Assistant text (typed responses)
          if(type === "response.text.delta"){
            const delta = msg.delta || "";
            if(delta) textBuffer += delta;
          }

          if(type === "response.text.done"){
            const text = (msg.text || textBuffer || "").trim();
            if(text){
              appendLog("ü§ñ Assistant: " + text);
              await sendToN8N("assistant", text);
            }
            textBuffer = "";
          }

          // 3) Assistant audio transcript (voice responses)
          if(type === "response.output_item.done"){
            if(!msg.item || !msg.item.content) return;
            for(const c of msg.item.content){
              if(c.type === "audio" && c.transcript){
                const t = c.transcript.trim();
                appendLog("ü§ñ Assistant (voice): " + t);
                await sendToN8N("assistant", t);
              }
            }
          }
        };

        // Offer/Answer with OpenAI Realtime
        const offer = await pc.createOffer();
        await pc.setLocalDescription(offer);

        const res = await fetch(
          `https://api.openai.com/v1/realtime?model=${model}`,
          {
            method:"POST",
            headers:{
              "Content-Type":"application/sdp",
              "Authorization": `Bearer ${token}`
            },
            body: offer.sdp
          }
        );

        const ans = await res.text();

        appendLog(`üõ∞Ô∏è Realtime SDP response status: ${res.status}`);
        appendLog(`üõ∞Ô∏è Realtime SDP response head: ${ans.slice(0,120).replace(/\n/g," ")}`);

        if(!res.ok || !ans.trim().startsWith("v=")){
          throw new Error("Realtime returned non-SDP: " + ans.slice(0,200));
        }

        await pc.setRemoteDescription({ type:"answer", sdp: ans });

        setStatus("üü¢ Connected ‚Äî type anytime. Click üéôÔ∏è Talk to start/stop voice.");
        appendLog("‚úÖ Connected to OpenAI realtime");

      }catch(err){
        setStatus("‚ùå " + err.message);
        appendLog("‚ùå Init error: " + err.message);
      }
    }

    initRealtime();
  </script>
</body>
</html>


