<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>aenimAI Realtime Voice Chat</title>
  <style>
    body{
      font-family: Arial, sans-serif;
      background-color:#121212;
      color:#f5f5f5;
      display:flex;
      flex-direction:column;
      align-items:center;
      justify-content:center;
      height:100vh;
      margin:0;
      text-align:center;
    }
    h1{ color:#1db954; margin-bottom:8px; }
    #status{ color:#ccc; font-size:16px; margin-bottom:8px; }

    #controlsRow{
      display:flex;
      gap:10px;
      align-items:center;
      justify-content:center;
      margin-top:12px;
      width:min(560px,92vw);
    }
    #inputRow{
      display:flex;
      gap:8px;
      align-items:center;
      justify-content:center;
      width:100%;
    }
    #textInput{
      flex:1;
      min-width:180px;
      padding:10px 12px;
      border-radius:10px;
      border:1px solid #333;
      background:rgba(0,0,0,0.35);
      color:#f5f5f5;
      outline:none;
    }
    #sendText{
      padding:10px 14px;
      border-radius:10px;
      border:1px solid #2b2b2b;
      background:#1db954;
      color:#0b0b0b;
      cursor:pointer;
      font-weight:600;
      white-space:nowrap;
    }
    #sendText:disabled{ opacity:.5; cursor:not-allowed; }

    /* NEW: mic toggle button */
    #micToggle{
      padding:10px 14px;
      border-radius:10px;
      border:1px solid #2b2b2b;
      background:#2a2a2a;
      color:#f5f5f5;
      cursor:pointer;
      font-weight:600;
      white-space:nowrap;
    }
    #micToggle.on{
      background:#1db954;
      color:#0b0b0b;
    }
    #micToggle:disabled{ opacity:.5; cursor:not-allowed; }

    #log{
      position:fixed;
      bottom:10px; left:10px; right:10px;
      max-height:35vh; overflow-y:auto;
      font-size:11px;
      background:rgba(0,0,0,0.75);
      padding:8px; border-radius:8px;
      border:1px solid #333;
      text-align:left; white-space:pre-wrap;
    }
  </style>
</head>

<body>
  <h1>üéôÔ∏è aenimAI Realtime Voice Chat</h1>
  <p id="status">Initializing...</p>

  <div id="controlsRow">
    <div id="inputRow">
      <input id="textInput" type="text" placeholder="Type a message‚Ä¶" autocomplete="off" />
      <button id="sendText" disabled>Send</button>
    </div>
    <button id="micToggle" disabled>üéôÔ∏è Talk</button>
  </div>

  <div id="log"></div>

  <script>
    const N8N_SESSION_WEBHOOK = "https://aenimai.app.n8n.cloud/webhook/realtime-ai";
    const N8N_LOG_WEBHOOK     = "https://aenimai.app.n8n.cloud/webhook/get-logs";

    const SESSION_ID = "sess_" + Math.random().toString(36).slice(2) + "_" + Date.now();

    const statusEl = document.getElementById("status");
    const logEl    = document.getElementById("log");

    const textInputEl = document.getElementById("textInput");
    const sendBtnEl   = document.getElementById("sendText");
    const micBtnEl    = document.getElementById("micToggle");

    function setStatus(t){ statusEl.innerText = t; }
    function appendLog(t){
      const ts = new Date().toISOString().slice(11,19);
      logEl.textContent += `[${ts}] ${t}\n`;
      logEl.scrollTop = logEl.scrollHeight;
    }

    async function sendToN8N(role, text){
      if(!text) return;
      const body = {
        source: "aenimAI-frontend",
        session_id: SESSION_ID,
        timestamp: new Date().toISOString(),
        role, text
      };
      try{
        await fetch(N8N_LOG_WEBHOOK,{
          method:"POST",
          headers:{ "Content-Type":"application/json" },
          body: JSON.stringify(body)
        });
        appendLog(`üì§ Sent to n8n [${role}]: ${text}`);
      }catch(err){
        appendLog("‚ö†Ô∏è Failed to send: " + err.message);
      }
    }

    async function getSession(){
      appendLog("üîÑ Requesting session from n8n...");
      const r = await fetch(N8N_SESSION_WEBHOOK);
      const txt = await r.text();
      appendLog("üì• Session: " + txt.slice(0,200) + "...");
      return JSON.parse(txt);
    }

    async function initRealtime(){
      try{
        const session = await getSession();
        const token = session.client_secret.value;
        const model = session.model;

        // --- WebRTC setup (NO MIC REQUEST YET) ---
        const pc = new RTCPeerConnection();

        const remoteAudio = document.createElement("audio");
        remoteAudio.autoplay = true;
        pc.ontrack = e => { remoteAudio.srcObject = e.streams[0]; };

        const dc = pc.createDataChannel("oai-events");

        // mic state
        let micStream = null;
        let micEnabled = false;

        // ‚úÖ Explicitly request assistant response in chosen modality
        function requestAssistantResponse(modalities){
          if(!dc || dc.readyState !== "open"){
            appendLog("‚ö†Ô∏è Data channel not open yet ‚Äî can't request response.");
            return;
          }
          dc.send(JSON.stringify({
            type: "response.create",
            response: { modalities } // ["text"] or ["audio"]
          }));
        }

        // NEW: enable/disable mic by adding/removing tracks
        async function enableMic(){
          if(micEnabled) return;
          try{
            micStream = await navigator.mediaDevices.getUserMedia({ audio:true });
            micStream.getTracks().forEach(t => pc.addTrack(t, micStream));
            micEnabled = true;
            micBtnEl.classList.add("on");
            micBtnEl.textContent = "üî¥ Talking (click to stop)";
            appendLog("üéôÔ∏è Mic enabled");

            // (Optional) tell user they can speak now:
            // appendLog("üó£Ô∏è Speak now...");
          }catch(err){
            appendLog("‚ùå Mic permission/error: " + err.message);
          }
        }

        function disableMic(){
          if(!micEnabled) return;
          try{
            if(micStream){
              micStream.getTracks().forEach(t => t.stop());
            }
          }catch(e){}
          micStream = null;
          micEnabled = false;
          micBtnEl.classList.remove("on");
          micBtnEl.textContent = "üéôÔ∏è Talk";
          appendLog("üéôÔ∏è Mic disabled");
        }

        function toggleMic(){
          if(!dc || dc.readyState !== "open"){
            appendLog("‚ö†Ô∏è Not connected yet ‚Äî can't toggle mic.");
            return;
          }
          if(micEnabled) disableMic();
          else enableMic();
        }

        dc.onopen = ()=>{
          appendLog("üì° Data channel open");
          sendBtnEl.disabled = false;
          micBtnEl.disabled  = false;
        };
        dc.onerror = (e)=>appendLog("‚ö†Ô∏è Data channel error " + (e?.message || ""));

        // ‚úÖ Typed input => user msg + TEXT response
        function sendTypedText(){
          const text = (textInputEl.value || "").trim();
          if(!text) return;

          if(!dc || dc.readyState !== "open"){
            appendLog("‚ö†Ô∏è Not connected yet ‚Äî can't send typed text.");
            return;
          }

          dc.send(JSON.stringify({
            type: "conversation.item.create",
            item: {
              type: "message",
              role: "user",
              content: [{ type: "input_text", text }]
            }
          }));

          appendLog("üßë User (typed): " + text);
          sendToN8N("user", text);

          requestAssistantResponse(["text"]);

          textInputEl.value = "";
          textInputEl.focus();
        }

        sendBtnEl.addEventListener("click", sendTypedText);
        textInputEl.addEventListener("keydown", (e)=>{
          if(e.key === "Enter") sendTypedText();
        });

        // NEW: mic toggle button
        micBtnEl.addEventListener("click", toggleMic);

        // Text buffer (optional)
        let textBuffer = "";

        dc.onmessage = async (event)=>{
          console.log("FULL RAW:", event.data);
          let msg;
          try { msg = JSON.parse(event.data); }
          catch { return; }

          const type = msg.type || "";

          // ===============================
          // 1) USER SPEECH TRANSCRIPTION
          // ===============================
          if(type === "conversation.item.input_audio_transcription.completed"){
            const text = (msg.transcript || "").trim();
            if(text){
              appendLog("üßë User: " + text);
              await sendToN8N("user", text);

              // Only request audio response if mic is currently enabled
              if(micEnabled){
                requestAssistantResponse(["audio"]);
              }
            }
          }

          // ===============================
          // 2) ASSISTANT TEXT EVENTS (typed mode)
          // ===============================
          if(type === "response.text.delta"){
            const delta = msg.delta || "";
            if(delta){
              textBuffer += delta;
              // keep it simple: show streaming as separate logs (or wait for done)
              // appendLog("ü§ñ Assistant (typing): " + delta);
            }
          }

          if(type === "response.text.done"){
            const text = (msg.text || textBuffer || "").trim();
            if(text){
              appendLog("ü§ñ Assistant: " + text);
              await sendToN8N("assistant", text);
            }
            textBuffer = "";
          }

          // ===============================
          // 3) ASSISTANT AUDIO TRANSCRIPT EVENTS (voice mode)
          // ===============================
          if(type === "response.output_item.done"){
            if(!msg.item || !msg.item.content) return;
            for(const c of msg.item.content){
              if(c.type === "audio" && c.transcript){
                const text = c.transcript.trim();
                appendLog("ü§ñ Assistant (voice): " + text);
                await sendToN8N("assistant", text);
              }
            }
          }
        };

        // WebRTC offer ‚Üí OpenAI
        const offer = await pc.createOffer();
        await pc.setLocalDescription(offer);

        const res = await fetch(
          `https://api.openai.com/v1/realtime?model=${model}`,
          {
            method:"POST",
            headers:{
              "Content-Type":"application/sdp",
              Authorization:`Bearer ${token}`
            },
            body: offer.sdp
          }
        );

        const ans = await res.text();
        await pc.setRemoteDescription({type:"answer", sdp: ans});

        setStatus("üü¢ Connected ‚Äî type anytime. Click üéôÔ∏è Talk to start/stop voice.");
        appendLog("‚úÖ Connected to OpenAI realtime");
        appendLog("üí° Tip: Start with typing; enable mic only when you want to talk.");

      }catch(err){
        setStatus("‚ùå " + err.message);
        appendLog("‚ùå Init error: " + err.message);
      }
    }

    initRealtime();
  </script>
</body>
</html>
